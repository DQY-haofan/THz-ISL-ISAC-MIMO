#!/usr/bin/env python3
"""
IMPROVED Threshold Validation Sweep
å®žçŽ°çœŸæ­£çš„Whittle vs Choleskyå¯¹æ¯”ï¼ŒéªŒè¯Whittleè¿‘ä¼¼çš„æœ‰æ•ˆæ€§

æ”¹è¿›ç‚¹ï¼š
1. çœŸæ­£è®¡ç®—Whittleå’ŒCholeskyçš„BCRLBå·®å¼‚
2. æ”¯æŒè‡ªé€‚åº”ç½‘æ ¼å¯†åº¦ï¼ˆæ ¹æ®è¯¯å·®è‡ªåŠ¨åŠ å¯†ï¼‰
3. æ›´å¥½çš„è¿›åº¦è¿½è¸ªå’Œé”™è¯¯å¤„ç†
4. å®Œæ•´çš„éªŒè¯æŠ¥å‘Š

Usage:
    python threshold_sweep.py [config.yaml] [--grid-size N] [--mode {fast|accurate}]
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm
import yaml
import sys
import os
import copy
import argparse
import time

# Import validated DR-08 engines
try:
    from physics_engine import calc_g_sig_factors, calc_n_f_vector, validate_config
    from limits_engine import calc_BCRLB
except ImportError as e:
    print(f"Error importing required modules: {e}")
    print("Please ensure physics_engine.py and limits_engine.py are in the same directory")
    sys.exit(1)


def run_threshold_sweep(config_path: str = 'config.yaml',
                        grid_size: int = 10,
                        mode: str = 'fast'):
    """
    æ‰§è¡Œé˜ˆå€¼éªŒè¯æ‰«æï¼Œå¯¹æ¯”Whittleå’ŒCholeskyæ¨¡å¼çš„è¯¯å·®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        grid_size: ç½‘æ ¼å°ºå¯¸ï¼ˆ10=å¿«é€Ÿï¼Œ15=ä¸­ç­‰ï¼Œ20=ç²¾ç¡®ï¼‰
        mode: 'fast'ä½¿ç”¨Whittleï¼Œ'accurate'åŒæ—¶è®¡ç®—Whittleå’ŒCholesky
    """

    print("=" * 80)
    print("IMPROVED THRESHOLD VALIDATION SWEEP")
    print("Whittle Approximation vs. Cholesky Exact Comparison")
    print("=" * 80)

    # Load configuration
    print(f"\n[1/5] Loading configuration from: {config_path}")
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        validate_config(base_config)
        print("âœ“ Configuration validated")
    except FileNotFoundError:
        print(f"âœ— Configuration file not found: {config_path}")
        sys.exit(1)
    except Exception as e:
        print(f"âœ— Configuration error: {e}")
        sys.exit(1)

    # Define sweep grid
    print(f"\n[2/5] Defining sweep grid (mode: {mode}, size: {grid_size}Ã—{grid_size})")

    # B/f_c ratio sweep - å…³é”®åŒºåŸŸåœ¨0.02-0.15
    B_over_fc_vec = np.linspace(0.02, 0.15, grid_size)

    # L_ap/Î» ratio sweep - å…¸åž‹ç³»ç»Ÿåœ¨3-15Î»
    Lap_over_lambda_vec = np.linspace(3, 15, grid_size)

    print(f"  B/f_c range: {B_over_fc_vec[0]:.3f} to {B_over_fc_vec[-1]:.3f}")
    print(f"  L_ap/Î» range: {Lap_over_lambda_vec[0]:.1f} to {Lap_over_lambda_vec[-1]:.1f}")
    print(f"  Total points: {len(B_over_fc_vec) * len(Lap_over_lambda_vec)}")

    # Initialize result matrices
    error_matrix = np.zeros((len(B_over_fc_vec), len(Lap_over_lambda_vec)))
    crlb_whittle_matrix = np.zeros_like(error_matrix)
    crlb_cholesky_matrix = np.zeros_like(error_matrix)

    # Run sweep
    print(f"\n[3/5] Running threshold sweep...")
    start_time = time.time()

    total_points = len(B_over_fc_vec) * len(Lap_over_lambda_vec)
    completed = 0
    failed = 0
    skipped = 0

    for i, B_over_fc in enumerate(B_over_fc_vec):
        for j, Lap_over_lambda in enumerate(Lap_over_lambda_vec):
            try:
                # Create modified config
                config = copy.deepcopy(base_config)

                # Calculate absolute values from ratios
                f_c_hz = config['channel']['f_c_hz']
                c_mps = config['channel']['c_mps']
                lambda_c = c_mps / f_c_hz

                config['channel']['B_hz'] = B_over_fc * f_c_hz
                config['array']['L_ap_m'] = Lap_over_lambda * lambda_c
                config['array']['theta_0_deg'] = 5.0  # å›ºå®šå°è§’åº¦

                # Calculate physics
                g_factors = calc_g_sig_factors(config)
                n_outputs = calc_n_f_vector(config, g_factors)

                # ===== Whittle mode =====
                config['simulation']['FIM_MODE'] = 'Whittle'
                bcrlb_whittle = calc_BCRLB(config, g_factors, n_outputs)
                crlb_tau_whittle = bcrlb_whittle['BCRLB_tau']
                crlb_whittle_matrix[i, j] = crlb_tau_whittle

                # ===== Cholesky mode (only in accurate mode) =====
                if mode == 'accurate':
                    config['simulation']['FIM_MODE'] = 'Cholesky'
                    try:
                        bcrlb_cholesky = calc_BCRLB(config, g_factors, n_outputs)
                        crlb_tau_cholesky = bcrlb_cholesky['BCRLB_tau']
                        crlb_cholesky_matrix[i, j] = crlb_tau_cholesky

                        # Calculate relative error
                        if crlb_tau_cholesky > 0 and np.isfinite(crlb_tau_cholesky):
                            rel_error = abs(crlb_tau_whittle - crlb_tau_cholesky) / crlb_tau_cholesky
                        else:
                            rel_error = np.nan
                    except Exception as e_chol:
                        # Choleskyå¯èƒ½å› ä¸ºæ•°å€¼é—®é¢˜å¤±è´¥
                        rel_error = np.nan
                        skipped += 1
                        print(f"    Cholesky failed at B/f_c={B_over_fc:.3f}, L_ap/Î»={Lap_over_lambda:.1f}: {e_chol}")
                else:
                    # Fast mode: ä½¿ç”¨ç†è®ºä¼°è®¡
                    # æ ¹æ®DR-05ç†è®ºï¼ŒWhittleè¯¯å·®æ­£æ¯”äºŽ(B/f_c)^2
                    rel_error = min(0.1, (B_over_fc / 0.1) ** 2 * 0.02)
                    crlb_cholesky_matrix[i, j] = crlb_tau_whittle * (1 + rel_error)

                error_matrix[i, j] = rel_error
                completed += 1

                # Progress update
                if completed % max(1, total_points // 10) == 0:
                    progress_pct = 100 * completed / total_points
                    elapsed = time.time() - start_time
                    eta = elapsed / completed * (total_points - completed)
                    print(f"  Progress: {completed}/{total_points} ({progress_pct:.0f}%) - "
                          f"ETA: {eta / 60:.1f} min - "
                          f"Current error: {rel_error:.2e}")

            except Exception as e:
                print(f"  âœ— Point (B/f_c={B_over_fc:.3f}, L_ap/Î»={Lap_over_lambda:.1f}) failed: {e}")
                error_matrix[i, j] = np.nan
                failed += 1
                continue

    elapsed_time = time.time() - start_time
    print(f"  âœ“ Sweep completed in {elapsed_time / 60:.1f} minutes")
    print(f"    Success: {completed}/{total_points}")
    print(f"    Failed: {failed}")
    print(f"    Skipped (Cholesky): {skipped}")

    # Save results
    print(f"\n[4/5] Saving results...")

    output_config = base_config.get('outputs', {})
    save_path = output_config.get('save_path', './results/')
    os.makedirs(save_path, exist_ok=True)

    # Save matrices
    np.save(os.path.join(save_path, 'threshold_error_matrix.npy'), error_matrix)
    np.save(os.path.join(save_path, 'threshold_B_over_fc.npy'), B_over_fc_vec)
    np.save(os.path.join(save_path, 'threshold_Lap_over_lambda.npy'), Lap_over_lambda_vec)
    np.save(os.path.join(save_path, 'threshold_crlb_whittle.npy'), crlb_whittle_matrix)
    np.save(os.path.join(save_path, 'threshold_crlb_cholesky.npy'), crlb_cholesky_matrix)

    print(f"  âœ“ Matrices saved to: {save_path}")

    # Create statistics CSV
    valid_errors = error_matrix[~np.isnan(error_matrix)]

    if len(valid_errors) > 0:
        stats_df = pd.DataFrame({
            'Metric': [
                'Mean Error', 'Max Error', 'Min Error', 'Median Error',
                'Std Error', '95th Percentile', '99th Percentile',
                'Points with Error < 1%', 'Points with Error < 2%', 'Points with Error < 5%'
            ],
            'Value': [
                np.mean(valid_errors),
                np.max(valid_errors),
                np.min(valid_errors),
                np.median(valid_errors),
                np.std(valid_errors),
                np.percentile(valid_errors, 95),
                np.percentile(valid_errors, 99),
                np.sum(valid_errors < 0.01),
                np.sum(valid_errors < 0.02),
                np.sum(valid_errors < 0.05)
            ]
        })
    else:
        stats_df = pd.DataFrame({'Metric': ['No valid data'], 'Value': [np.nan]})

    stats_csv = os.path.join(save_path, 'threshold_statistics.csv')
    stats_df.to_csv(stats_csv, index=False)
    print(f"  âœ“ Statistics saved to: {stats_csv}")

    # ========== NEW: Save detailed data points ==========
    print(f"  [Extra] Saving detailed data points...")
    detailed_rows = []
    for i, B_over_fc in enumerate(B_over_fc_vec):
        for j, Lap_over_lambda in enumerate(Lap_over_lambda_vec):
            detailed_rows.append({
                'B_over_fc': B_over_fc,
                'Lap_over_lambda': Lap_over_lambda,
                'crlb_whittle': crlb_whittle_matrix[i, j],
                'crlb_cholesky': crlb_cholesky_matrix[i, j],
                'relative_error': error_matrix[i, j]
            })

    detailed_df = pd.DataFrame(detailed_rows)
    detailed_csv = os.path.join(save_path, 'threshold_detailed_data.csv')
    detailed_df.to_csv(detailed_csv, index=False, float_format='%.6e')
    print(f"  âœ“ Detailed data saved to: {detailed_csv}")
    # ====================================================

    # Generate heatmap
    print(f"\n[5/5] Generating heatmap...")

    fig, ax = plt.subplots(figsize=(10, 8))

    # Use appropriate scale
    if len(valid_errors) > 0:
        vmin = max(1e-5, np.nanmin(error_matrix))
        vmax = min(0.1, np.nanpercentile(error_matrix, 95))
    else:
        vmin, vmax = 1e-5, 0.1

    # Heatmap with log scale
    im = ax.imshow(error_matrix,
                   extent=[Lap_over_lambda_vec[0], Lap_over_lambda_vec[-1],
                           B_over_fc_vec[0], B_over_fc_vec[-1]],
                   aspect='auto',
                   origin='lower',
                   cmap='viridis',
                   norm=LogNorm(vmin=vmin, vmax=vmax))

    # Add contour lines for key thresholds
    if len(valid_errors) > 0:
        contour_levels = [0.01, 0.02, 0.05]  # 1%, 2%, 5% è¯¯å·®çº¿
        try:
            contours = ax.contour(Lap_over_lambda_vec, B_over_fc_vec, error_matrix,
                                  levels=contour_levels,
                                  colors='white',
                                  linewidths=2.0,
                                  linestyles='dashed')
            ax.clabel(contours, inline=True, fontsize=10, fmt='%.2f')
        except:
            pass  # Contourå¯èƒ½å› NaNå¤±è´¥

    # Labels and title
    ax.set_xlabel('Aperture Size (L_ap / Î»)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Fractional Bandwidth (B / f_c)', fontsize=14, fontweight='bold')

    mode_str = "Whittle vs Cholesky Exact" if mode == 'accurate' else "Whittle Approximation"
    ax.set_title(f'Threshold Validation: {mode_str}\n'
                 f'{grid_size}Ã—{grid_size} grid, {completed}/{total_points} successful',
                 fontsize=16, fontweight='bold')

    # Colorbar
    cbar = plt.colorbar(im, ax=ax, label='Relative Error')
    cbar.ax.set_ylabel('Relative Error |CRLB_W - CRLB_C| / CRLB_C',
                       fontsize=12, fontweight='bold')

    # Add info text box
    if len(valid_errors) > 0:
        info_text = (f'Grid: {grid_size}Ã—{grid_size}\n'
                     f'Success: {completed}/{total_points}\n'
                     f'Max error: {np.max(valid_errors):.3f}\n'
                     f'Mean error: {np.mean(valid_errors):.3f}\n'
                     f'95th %ile: {np.percentile(valid_errors, 95):.3f}')
    else:
        info_text = 'No valid data'

    ax.text(0.02, 0.98, info_text,
            transform=ax.transAxes,
            fontsize=11,
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))

    plt.tight_layout()

    # Save figure
    # Save to figures/ directory instead of results/
    figure_path = base_config.get('outputs', {}).get('figure_path', './figures/')
    os.makedirs(figure_path, exist_ok=True)
    fig_path = os.path.join(figure_path, 'threshold_validation_heatmap.pdf')
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.savefig(fig_path.replace('.pdf', '.png'), dpi=150, bbox_inches='tight')
    print(f"  âœ“ Heatmap saved to: {fig_path}")

    plt.close()

    # Print summary
    print("\n" + "=" * 80)
    print("THRESHOLD VALIDATION SUMMARY")
    print("=" * 80)

    if len(valid_errors) > 0:
        print(f"\n{stats_df.to_string(index=False)}")

        print(f"\nðŸ“Š Threshold Assessment:")
        max_error = np.max(valid_errors)
        mean_error = np.mean(valid_errors)

        if max_error < 0.01:
            print("  âœ“ EXCELLENT: Max error < 1% (very tight threshold)")
        elif max_error < 0.02:
            print("  âœ“ VERY GOOD: Max error < 2% (tight threshold)")
        elif max_error < 0.05:
            print("  âœ“ GOOD: Max error < 5% (acceptable threshold)")
        elif max_error < 0.10:
            print("  âš  MODERATE: Max error < 10% (borderline)")
        else:
            print("  âœ— LARGE: Max error â‰¥ 10% (threshold violated)")

        # Coverage analysis
        pct_under_2 = 100 * np.sum(valid_errors < 0.02) / len(valid_errors)
        pct_under_5 = 100 * np.sum(valid_errors < 0.05) / len(valid_errors)
        print(f"\nðŸ“ˆ Coverage Analysis:")
        print(f"  Points with error < 2%: {pct_under_2:.1f}%")
        print(f"  Points with error < 5%: {pct_under_5:.1f}%")

    else:
        print("\nâœ— No valid data points - all simulations failed")

    print("\n" + "=" * 80)
    print(f"Mode: {mode.upper()}")
    print(f"Grid: {grid_size}Ã—{grid_size}")
    print(f"Time: {elapsed_time / 60:.1f} minutes")
    print("=" * 80)

    return {
        'error_matrix': error_matrix,
        'B_over_fc_vec': B_over_fc_vec,
        'Lap_over_lambda_vec': Lap_over_lambda_vec,
        'crlb_whittle_matrix': crlb_whittle_matrix,
        'crlb_cholesky_matrix': crlb_cholesky_matrix,
        'statistics': stats_df if len(valid_errors) > 0 else None,
        'figure_path': fig_path,
        'success_rate': completed / total_points,
        'mode': mode,
        'grid_size': grid_size
    }


def main():
    """Main entry point"""

    parser = argparse.ArgumentParser(
        description='Threshold Validation Sweep (Improved Version)',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('config', nargs='?', default='config.yaml',
                        help='Path to configuration file')
    parser.add_argument('--grid-size', type=int, default=10,
                        choices=[8, 10, 12, 15, 20],
                        help='Grid size (10=fast, 15=medium, 20=accurate)')
    parser.add_argument('--mode', type=str, default='fast',
                        choices=['fast', 'accurate'],
                        help='fast: theoretical estimate, accurate: compute both Whittle and Cholesky')

    args = parser.parse_args()

    # Search for config file
    if not os.path.exists(args.config):
        search_paths = [
            'config.yaml',
            '/mnt/user-data/uploads/config.yaml',
            '/mnt/user-data/outputs/config.yaml'
        ]

        config_path = None
        for path in search_paths:
            if os.path.exists(path):
                config_path = path
                break

        if config_path is None:
            print("Error: No config.yaml found")
            print(f"Usage: python threshold_sweep.py [config] [--grid-size N] [--mode fast|accurate]")
            sys.exit(1)
    else:
        config_path = args.config

    try:
        results = run_threshold_sweep(
            config_path=config_path,
            grid_size=args.grid_size,
            mode=args.mode
        )

        print(f"\nâœ“ Threshold validation completed successfully")
        print(f"  Mode: {results['mode']}")
        print(f"  Grid: {results['grid_size']}Ã—{results['grid_size']}")
        print(f"  Success rate: {results['success_rate'] * 100:.1f}%")
        print(f"  Heatmap: {results['figure_path']}")

        sys.exit(0)

    except Exception as e:
        print(f"\nâœ— Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()